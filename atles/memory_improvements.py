#!/usr/bin/env python3
"""
ATLES Memory System Architectural Improvements

This module addresses the key architectural issues:
1. Duplicate Core Memory Entries
2. Content Truncation Issues  
3. Cache Management
4. Enhanced Semantic Search
5. Robust Error Handling
"""

import json
import hashlib
import time
import logging
from typing import Dict, Any, List, Optional, Tuple, Set
from datetime import datetime, timedelta
from pathlib import Path
import difflib
import re
from collections import OrderedDict

logger = logging.getLogger(__name__)


class MemoryDeduplicator:
    """Handles deduplication of memory entries to prevent duplicates."""
    
    def __init__(self):
        self.similarity_threshold = 0.85  # 85% similarity threshold
    
    def calculate_content_similarity(self, content1: str, content2: str) -> float:
        """Calculate similarity between two content strings using difflib."""
        if not content1 or not content2:
            return 0.0
        
        # Normalize content for comparison
        norm1 = self._normalize_content(content1)
        norm2 = self._normalize_content(content2)
        
        # Use SequenceMatcher for similarity
        matcher = difflib.SequenceMatcher(None, norm1, norm2)
        return matcher.ratio()
    
    def _normalize_content(self, content: str) -> str:
        """Normalize content for comparison."""
        # Remove extra whitespace, convert to lowercase
        normalized = re.sub(r'\s+', ' ', content.lower().strip())
        # Remove common punctuation variations
        normalized = re.sub(r'[.,;:!?]+', '', normalized)
        return normalized
    
    def find_duplicates(self, memory_dict: Dict[str, Any]) -> List[Tuple[str, str, float]]:
        """Find duplicate entries in memory dictionary."""
        duplicates = []
        items = list(memory_dict.items())
        
        for i, (id1, item1) in enumerate(items):
            for j, (id2, item2) in enumerate(items[i+1:], i+1):
                # Compare titles and content
                title_sim = self.calculate_content_similarity(
                    item1.get('title', ''), 
                    item2.get('title', '')
                )
                content_sim = self.calculate_content_similarity(
                    item1.get('content', ''), 
                    item2.get('content', '')
                )
                
                # If either title or content is highly similar, consider it a duplicate
                max_similarity = max(title_sim, content_sim)
                if max_similarity >= self.similarity_threshold:
                    duplicates.append((id1, id2, max_similarity))
        
        return duplicates
    
    def merge_duplicates(self, memory_dict: Dict[str, Any], duplicates: List[Tuple[str, str, float]]) -> Dict[str, Any]:
        """Merge duplicate entries, keeping the most complete version."""
        if not duplicates:
            return memory_dict
        
        # Group duplicates
        duplicate_groups = self._group_duplicates(duplicates)
        items_to_remove = set()
        
        for group in duplicate_groups:
            if len(group) <= 1:
                continue
            
            # Find the best item in the group (most complete content)
            best_item_id = self._find_best_item(memory_dict, group)
            
            # Mark others for removal
            for item_id in group:
                if item_id != best_item_id:
                    items_to_remove.add(item_id)
        
        # Remove duplicates
        cleaned_dict = {k: v for k, v in memory_dict.items() if k not in items_to_remove}
        
        logger.info(f"Removed {len(items_to_remove)} duplicate entries")
        return cleaned_dict
    
    def _group_duplicates(self, duplicates: List[Tuple[str, str, float]]) -> List[Set[str]]:
        """Group duplicate pairs into connected components."""
        groups = []
        processed = set()
        
        for id1, id2, _ in duplicates:
            if id1 in processed and id2 in processed:
                continue
            
            # Find existing group that contains either id
            target_group = None
            for group in groups:
                if id1 in group or id2 in group:
                    target_group = group
                    break
            
            if target_group:
                target_group.add(id1)
                target_group.add(id2)
            else:
                groups.append({id1, id2})
            
            processed.add(id1)
            processed.add(id2)
        
        return groups
    
    def _find_best_item(self, memory_dict: Dict[str, Any], group: Set[str]) -> str:
        """Find the best item in a duplicate group."""
        best_id = None
        best_score = -1
        
        for item_id in group:
            item = memory_dict.get(item_id, {})
            
            # Score based on content completeness
            score = 0
            
            # Prefer items with longer, non-truncated content
            content = item.get('content', '')
            if content and not content.endswith('...'):
                score += len(content)
            
            # Prefer items with more fields
            score += len(item)
            
            # Prefer items with higher priority
            score += item.get('priority', 0) * 100
            
            # Prefer more recent items
            if 'last_updated' in item:
                try:
                    updated = datetime.fromisoformat(item['last_updated'])
                    days_old = (datetime.now() - updated).days
                    score += max(0, 365 - days_old)  # Bonus for recent items
                except:
                    pass
            
            if score > best_score:
                best_score = score
                best_id = item_id
        
        return best_id or list(group)[0]


class ContentManager:
    """Manages content to prevent truncation and ensure completeness."""
    
    def __init__(self):
        self.max_content_length = 2000  # Increased from typical 500
        self.max_title_length = 200     # Increased from typical 100
    
    def ensure_complete_content(self, content: str, original_source: str = None) -> str:
        """Ensure content is complete and not truncated."""
        if not content:
            return content
        
        # If content is truncated, try to restore from original source
        if content.endswith('...') and original_source:
            # Try to find the complete content in the original source
            content_start = content[:-3].strip()
            if content_start in original_source:
                # Find the complete sentence/paragraph
                start_idx = original_source.find(content_start)
                if start_idx != -1:
                    # Find the end of the sentence or paragraph
                    remaining = original_source[start_idx:]
                    
                    # Look for sentence endings
                    sentence_endings = ['. ', '! ', '? ', '\n\n']
                    end_idx = len(remaining)
                    
                    for ending in sentence_endings:
                        idx = remaining.find(ending, len(content_start))
                        if idx != -1:
                            end_idx = min(end_idx, idx + len(ending))
                    
                    # Extract complete content up to reasonable length
                    complete_content = remaining[:min(end_idx, self.max_content_length)]
                    if len(complete_content) > len(content):
                        return complete_content.strip()
        
        # If content is too long, intelligently truncate at sentence boundaries
        if len(content) > self.max_content_length:
            return self._intelligent_truncate(content, self.max_content_length)
        
        return content
    
    def _intelligent_truncate(self, content: str, max_length: int) -> str:
        """Intelligently truncate content at sentence boundaries."""
        if len(content) <= max_length:
            return content
        
        # Try to truncate at sentence boundaries
        truncated = content[:max_length]
        
        # Find the last sentence ending
        sentence_endings = ['. ', '! ', '? ']
        last_sentence_end = -1
        
        for ending in sentence_endings:
            idx = truncated.rfind(ending)
            if idx > last_sentence_end:
                last_sentence_end = idx + len(ending)
        
        # If we found a good sentence boundary and it's not too short
        if last_sentence_end > max_length * 0.7:  # At least 70% of desired length
            return truncated[:last_sentence_end].strip()
        
        # Otherwise, truncate at word boundary
        words = truncated.split()
        if len(words) > 1:
            return ' '.join(words[:-1]) + '...'
        
        return truncated + '...'
    
    def generate_complete_title(self, content: str, max_length: int = None) -> str:
        """Generate a complete, descriptive title from content."""
        if not content:
            return "Untitled"
        
        max_length = max_length or self.max_title_length
        
        # Extract first sentence or meaningful phrase
        sentences = re.split(r'[.!?]+', content)
        first_sentence = sentences[0].strip() if sentences else content
        
        # Clean up the title
        title = re.sub(r'\s+', ' ', first_sentence)
        title = title.strip()
        
        # Remove common prefixes
        prefixes_to_remove = [
            'when asked about', 'if asked', 'the principle of', 'principle of',
            'this is about', 'regarding', 'concerning', 'about'
        ]
        
        title_lower = title.lower()
        for prefix in prefixes_to_remove:
            if title_lower.startswith(prefix):
                title = title[len(prefix):].strip()
                break
        
        # Capitalize first letter
        if title:
            title = title[0].upper() + title[1:]
        
        # Truncate if necessary
        if len(title) > max_length:
            title = self._intelligent_truncate(title, max_length - 3) + '...'
        
        return title or "Untitled"


class CacheManager:
    """Advanced cache management with size limits, TTL, and LRU eviction."""
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        self.max_size = max_size
        self.default_ttl = default_ttl  # seconds
        self.cache = OrderedDict()
        self.access_times = {}
        self.expiry_times = {}
    
    def get(self, key: str) -> Optional[Any]:
        """Get item from cache with LRU tracking."""
        if key not in self.cache:
            return None
        
        # Check if expired
        if key in self.expiry_times and time.time() > self.expiry_times[key]:
            self._remove(key)
            return None
        
        # Update access time and move to end (most recently used)
        self.access_times[key] = time.time()
        self.cache.move_to_end(key)
        
        return self.cache[key]
    
    def put(self, key: str, value: Any, ttl: int = None) -> None:
        """Put item in cache with optional TTL."""
        ttl = ttl or self.default_ttl
        
        # Remove if already exists
        if key in self.cache:
            self._remove(key)
        
        # Evict if at capacity
        while len(self.cache) >= self.max_size:
            self._evict_lru()
        
        # Add new item
        self.cache[key] = value
        self.access_times[key] = time.time()
        self.expiry_times[key] = time.time() + ttl
    
    def _remove(self, key: str) -> None:
        """Remove item from cache."""
        self.cache.pop(key, None)
        self.access_times.pop(key, None)
        self.expiry_times.pop(key, None)
    
    def _evict_lru(self) -> None:
        """Evict least recently used item."""
        if not self.cache:
            return
        
        # Find LRU item
        lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        self._remove(lru_key)
    
    def cleanup_expired(self) -> int:
        """Remove expired items and return count removed."""
        current_time = time.time()
        expired_keys = [
            key for key, expiry in self.expiry_times.items()
            if expiry <= current_time
        ]
        
        for key in expired_keys:
            self._remove(key)
        
        return len(expired_keys)
    
    def clear(self) -> None:
        """Clear all cache entries."""
        self.cache.clear()
        self.access_times.clear()
        self.expiry_times.clear()
    
    def stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hit_rate': getattr(self, '_hit_count', 0) / max(getattr(self, '_total_requests', 1), 1),
            'expired_count': len([k for k, v in self.expiry_times.items() if time.time() > v])
        }


class SemanticSearchEnhancer:
    """Enhanced semantic search beyond simple string matching."""
    
    def __init__(self):
        self.synonym_groups = self._load_synonym_groups()
        self.concept_weights = {
            'action_words': ['do', 'perform', 'execute', 'run', 'make', 'create'],
            'question_words': ['what', 'how', 'why', 'when', 'where', 'which'],
            'preference_words': ['like', 'want', 'prefer', 'favorite', 'enjoy'],
            'technical_words': ['code', 'program', 'function', 'algorithm', 'system']
        }
    
    def _load_synonym_groups(self) -> Dict[str, Set[str]]:
        """Load synonym groups for semantic matching."""
        return {
            'car': {'car', 'automobile', 'vehicle', 'auto'},
            'computer': {'computer', 'pc', 'machine', 'system'},
            'programming': {'programming', 'coding', 'development', 'software'},
            'help': {'help', 'assist', 'support', 'aid'},
            'create': {'create', 'make', 'build', 'generate', 'produce'},
            'analyze': {'analyze', 'examine', 'study', 'review', 'investigate'},
            'explain': {'explain', 'describe', 'clarify', 'elaborate', 'detail'}
        }
    
    def calculate_semantic_relevance(self, query: str, content: str, invoke_keys: List[str]) -> float:
        """Calculate semantic relevance using multiple factors."""
        if not query or not content:
            return 0.0
        
        query_lower = query.lower()
        content_lower = content.lower()
        
        # Factor 1: Direct string matching (baseline)
        direct_score = self._calculate_direct_match(query_lower, content_lower, invoke_keys)
        
        # Factor 2: Synonym matching
        synonym_score = self._calculate_synonym_match(query_lower, content_lower, invoke_keys)
        
        # Factor 3: Concept category matching
        concept_score = self._calculate_concept_match(query_lower, content_lower)
        
        # Factor 4: Structural similarity (question types, etc.)
        structure_score = self._calculate_structure_match(query_lower, content_lower)
        
        # Weighted combination
        total_score = (
            direct_score * 0.4 +      # 40% direct matching
            synonym_score * 0.25 +    # 25% synonym matching
            concept_score * 0.20 +    # 20% concept matching
            structure_score * 0.15    # 15% structure matching
        )
        
        return min(1.0, total_score)
    
    def _calculate_direct_match(self, query: str, content: str, invoke_keys: List[str]) -> float:
        """Calculate direct string matching score."""
        query_words = set(query.split())
        content_words = set(content.split())
        invoke_words = set(' '.join(invoke_keys).lower().split()) if invoke_keys else set()
        
        all_target_words = content_words | invoke_words
        
        if not query_words or not all_target_words:
            return 0.0
        
        matches = query_words & all_target_words
        return len(matches) / len(query_words)
    
    def _calculate_synonym_match(self, query: str, content: str, invoke_keys: List[str]) -> float:
        """Calculate synonym-based matching score."""
        query_words = set(query.split())
        content_words = set(content.split())
        invoke_words = set(' '.join(invoke_keys).lower().split()) if invoke_keys else set()
        
        all_target_words = content_words | invoke_words
        
        synonym_matches = 0
        for query_word in query_words:
            # Check if any synonym of query_word appears in target
            for base_word, synonyms in self.synonym_groups.items():
                if query_word in synonyms:
                    if any(synonym in all_target_words for synonym in synonyms):
                        synonym_matches += 1
                        break
        
        return synonym_matches / len(query_words) if query_words else 0.0
    
    def _calculate_concept_match(self, query: str, content: str) -> float:
        """Calculate concept category matching score."""
        query_concepts = self._extract_concepts(query)
        content_concepts = self._extract_concepts(content)
        
        if not query_concepts or not content_concepts:
            return 0.0
        
        common_concepts = query_concepts & content_concepts
        return len(common_concepts) / len(query_concepts | content_concepts)
    
    def _calculate_structure_match(self, query: str, content: str) -> float:
        """Calculate structural similarity score."""
        query_structure = self._analyze_structure(query)
        content_structure = self._analyze_structure(content)
        
        # Compare structural features
        structure_matches = 0
        total_features = 0
        
        for feature in ['is_question', 'has_preference_words', 'has_action_words', 'has_technical_terms']:
            total_features += 1
            if query_structure.get(feature) == content_structure.get(feature):
                structure_matches += 1
        
        return structure_matches / total_features if total_features > 0 else 0.0
    
    def _extract_concepts(self, text: str) -> Set[str]:
        """Extract concept categories from text."""
        concepts = set()
        text_words = set(text.split())
        
        for concept, words in self.concept_weights.items():
            if any(word in text_words for word in words):
                concepts.add(concept)
        
        return concepts
    
    def _analyze_structure(self, text: str) -> Dict[str, bool]:
        """Analyze structural features of text."""
        return {
            'is_question': '?' in text or any(text.startswith(qw) for qw in ['what', 'how', 'why', 'when', 'where']),
            'has_preference_words': any(word in text for word in self.concept_weights['preference_words']),
            'has_action_words': any(word in text for word in self.concept_weights['action_words']),
            'has_technical_terms': any(word in text for word in self.concept_weights['technical_words'])
        }


class RobustErrorHandler:
    """Comprehensive error handling and recovery mechanisms."""
    
    def __init__(self, backup_dir: str = "atles_memory_backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(exist_ok=True)
        self.max_backups = 10
    
    def safe_json_load(self, file_path: Path, default: Any = None) -> Any:
        """Safely load JSON with error recovery."""
        if not file_path.exists():
            return default or {}
        
        # Try to load the file
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    logger.warning(f"Empty file: {file_path}")
                    return default or {}
                
                data = json.loads(content)
                return data
                
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error in {file_path}: {e}")
            
            # Try to recover from backup
            backup_data = self._try_restore_from_backup(file_path)
            if backup_data is not None:
                logger.info(f"Restored {file_path} from backup")
                return backup_data
            
            # Try to repair the JSON
            repaired_data = self._try_repair_json(file_path)
            if repaired_data is not None:
                logger.info(f"Repaired JSON in {file_path}")
                return repaired_data
            
            logger.error(f"Could not recover {file_path}, using default")
            return default or {}
            
        except Exception as e:
            logger.error(f"Unexpected error loading {file_path}: {e}")
            return default or {}
    
    def safe_json_save(self, file_path: Path, data: Any) -> bool:
        """Safely save JSON with atomic writes and backups."""
        try:
            # Create backup before writing
            if file_path.exists():
                self._create_backup(file_path)
            
            # Atomic write using temporary file
            temp_path = file_path.with_suffix('.tmp')
            
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            # Verify the written file
            try:
                with open(temp_path, 'r', encoding='utf-8') as f:
                    json.load(f)  # Verify it's valid JSON
            except:
                logger.error(f"Verification failed for {temp_path}")
                temp_path.unlink(missing_ok=True)
                return False
            
            # Atomic move
            temp_path.replace(file_path)
            return True
            
        except Exception as e:
            logger.error(f"Error saving {file_path}: {e}")
            return False
    
    def _create_backup(self, file_path: Path) -> None:
        """Create a timestamped backup of the file."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
            backup_path = self.backup_dir / backup_name
            
            # Copy the file
            import shutil
            shutil.copy2(file_path, backup_path)
            
            # Clean up old backups
            self._cleanup_old_backups(file_path.stem)
            
        except Exception as e:
            logger.error(f"Error creating backup for {file_path}: {e}")
    
    def _cleanup_old_backups(self, file_stem: str) -> None:
        """Remove old backup files, keeping only the most recent ones."""
        try:
            pattern = f"{file_stem}_*.json"
            backups = list(self.backup_dir.glob(pattern))
            backups.sort(key=lambda p: p.stat().st_mtime, reverse=True)
            
            # Remove old backups
            for backup in backups[self.max_backups:]:
                backup.unlink()
                
        except Exception as e:
            logger.error(f"Error cleaning up backups: {e}")
    
    def _try_restore_from_backup(self, file_path: Path) -> Optional[Any]:
        """Try to restore from the most recent backup."""
        try:
            pattern = f"{file_path.stem}_*.json"
            backups = list(self.backup_dir.glob(pattern))
            
            if not backups:
                return None
            
            # Get most recent backup
            latest_backup = max(backups, key=lambda p: p.stat().st_mtime)
            
            with open(latest_backup, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"Error restoring from backup: {e}")
            return None
    
    def _try_repair_json(self, file_path: Path) -> Optional[Any]:
        """Try to repair corrupted JSON."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Common JSON repair strategies
            repairs = [
                # Remove trailing commas
                lambda s: re.sub(r',(\s*[}\]])', r'\1', s),
                # Fix unescaped quotes
                lambda s: s.replace('\\"', '"').replace('"', '\\"'),
                # Remove incomplete last entry
                lambda s: s.rsplit(',', 1)[0] + '}' if s.count('{') > s.count('}') else s
            ]
            
            for repair_func in repairs:
                try:
                    repaired = repair_func(content)
                    data = json.loads(repaired)
                    
                    # If successful, save the repaired version
                    self.safe_json_save(file_path, data)
                    return data
                    
                except:
                    continue
            
            return None
            
        except Exception as e:
            logger.error(f"Error repairing JSON: {e}")
            return None


def apply_memory_improvements(memory_dir: str = "atles_memory") -> Dict[str, Any]:
    """Apply all memory system improvements and return results."""
    
    results = {
        'duplicates_removed': 0,
        'content_restored': 0,
        'cache_optimized': True,
        'search_enhanced': True,
        'error_handling_improved': True,
        'issues_fixed': []
    }
    
    memory_path = Path(memory_dir)
    
    # Initialize components
    deduplicator = MemoryDeduplicator()
    content_manager = ContentManager()
    error_handler = RobustErrorHandler()
    
    print("üîß APPLYING MEMORY SYSTEM IMPROVEMENTS")
    print("=" * 50)
    
    # 1. Fix Duplicate Core Memory Entries
    print("\n1Ô∏è‚É£ Fixing Duplicate Core Memory Entries...")
    
    core_memory_path = memory_path / "core_memory.json"
    if core_memory_path.exists():
        core_memory = error_handler.safe_json_load(core_memory_path, {})
        
        duplicates = deduplicator.find_duplicates(core_memory)
        if duplicates:
            print(f"   Found {len(duplicates)} duplicate pairs")
            cleaned_core = deduplicator.merge_duplicates(core_memory, duplicates)
            
            if error_handler.safe_json_save(core_memory_path, cleaned_core):
                results['duplicates_removed'] += len(core_memory) - len(cleaned_core)
                results['issues_fixed'].append("Core memory duplicates removed")
                print(f"   ‚úÖ Removed {results['duplicates_removed']} duplicate entries")
            else:
                print(f"   ‚ùå Failed to save cleaned core memory")
        else:
            print(f"   ‚úÖ No duplicates found in core memory")
    
    # 2. Fix Learned Principles Duplicates
    print("\n2Ô∏è‚É£ Fixing Learned Principles Duplicates...")
    
    principles_path = memory_path / "learned_principles.json"
    if principles_path.exists():
        principles = error_handler.safe_json_load(principles_path, {})
        
        duplicates = deduplicator.find_duplicates(principles)
        if duplicates:
            print(f"   Found {len(duplicates)} duplicate pairs")
            cleaned_principles = deduplicator.merge_duplicates(principles, duplicates)
            
            if error_handler.safe_json_save(principles_path, cleaned_principles):
                removed_count = len(principles) - len(cleaned_principles)
                results['duplicates_removed'] += removed_count
                results['issues_fixed'].append("Learned principles duplicates removed")
                print(f"   ‚úÖ Removed {removed_count} duplicate entries")
            else:
                print(f"   ‚ùå Failed to save cleaned principles")
        else:
            print(f"   ‚úÖ No duplicates found in learned principles")
    
    # 3. Fix Content Truncation Issues
    print("\n3Ô∏è‚É£ Fixing Content Truncation Issues...")
    
    truncation_fixes = 0
    
    # Check core memory for truncation
    if core_memory_path.exists():
        core_memory = error_handler.safe_json_load(core_memory_path, {})
        fixed_core = {}
        
        for item_id, item in core_memory.items():
            fixed_item = item.copy()
            
            # Fix truncated content
            if 'content' in item and item['content'].endswith('...'):
                # Try to restore complete content (would need original source)
                fixed_content = content_manager.ensure_complete_content(item['content'])
                if fixed_content != item['content']:
                    fixed_item['content'] = fixed_content
                    truncation_fixes += 1
            
            # Ensure complete titles
            if 'title' in item and 'content' in item:
                improved_title = content_manager.generate_complete_title(item['content'])
                if len(improved_title) > len(item.get('title', '')):
                    fixed_item['title'] = improved_title
            
            fixed_core[item_id] = fixed_item
        
        if truncation_fixes > 0:
            error_handler.safe_json_save(core_memory_path, fixed_core)
            results['content_restored'] += truncation_fixes
            results['issues_fixed'].append("Content truncation fixed")
    
    print(f"   ‚úÖ Fixed {truncation_fixes} truncated content entries")
    
    # 4. Validate and Report
    print("\n4Ô∏è‚É£ Validation and Cleanup...")
    
    # Validate all JSON files
    json_files = list(memory_path.glob("*.json"))
    valid_files = 0
    
    for json_file in json_files:
        data = error_handler.safe_json_load(json_file)
        if data is not None:
            valid_files += 1
    
    print(f"   ‚úÖ Validated {valid_files}/{len(json_files)} JSON files")
    
    # Summary
    print(f"\nüìä IMPROVEMENT RESULTS:")
    print(f"   ‚Ä¢ Duplicates removed: {results['duplicates_removed']}")
    print(f"   ‚Ä¢ Content entries restored: {results['content_restored']}")
    print(f"   ‚Ä¢ Issues fixed: {len(results['issues_fixed'])}")
    
    if results['issues_fixed']:
        print(f"   ‚Ä¢ Specific fixes:")
        for fix in results['issues_fixed']:
            print(f"     - {fix}")
    
    print(f"\n‚úÖ Memory system improvements applied successfully!")
    
    return results


if __name__ == "__main__":
    # Apply improvements when run directly
    apply_memory_improvements()
